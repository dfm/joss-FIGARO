{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84011b3",
   "metadata": {},
   "source": [
    "# FIGARO: an introductive guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab8b9b1",
   "metadata": {},
   "source": [
    "This notebook shows how to use FIGARO, *Fast Inference for GW Astronomy, Research & Observations*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89180ee",
   "metadata": {},
   "source": [
    "## 1D probability density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595edf42",
   "metadata": {},
   "source": [
    "We will start from a simple problem: inferring a 1D probability density given a set of samples drawn from it.\n",
    "Let's draw some samples from a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d689e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, uniform\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "mu = 30\n",
    "sigma = 3\n",
    "n_samps = 1000\n",
    "dist = norm(mu, sigma) \n",
    "\n",
    "samples = dist.rvs(n_samps)\n",
    "\n",
    "n, b, p = plt.hist(samples, bins = int(np.sqrt(len(samples))), histtype = 'step', density = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee189de4",
   "metadata": {},
   "source": [
    "FIGARO contains a class designed to infer probability densities given a set of samples.\n",
    "\n",
    "In order to instantiate the class, we need to specify the boundaries of the distribution.\n",
    "We will assume that our probability density is bounded between 10 and 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536cf3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from figaro.mixture import DPGMM\n",
    "\n",
    "x_min = 10\n",
    "x_max = 50\n",
    "\n",
    "mix = DPGMM([[x_min, x_max]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f33dc36",
   "metadata": {},
   "source": [
    "Please note that the boundaries must be passed as a 2D array. This is to ensure that the very same syntax holds for multidimensional distributions too.\n",
    "\n",
    "The idea is that the algorithm *learns* the shape of the probability density from the available samples, one at a time: every new sample adds a piece of information to the inference. Therefore, we need to pass the samples to our mixture one at a time in order to draw a single realisation of the Dirichlet Process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4872b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in tqdm(samples):\n",
    "    mix.add_new_point(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2c5b4",
   "metadata": {},
   "source": [
    "Now that our mixture knows the shape of the distribution, we can build the probability density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea6dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = mix.build_mixture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d5eff",
   "metadata": {},
   "source": [
    "Before starting again with a new inference the mixture must be initialised, otherwise it will remember the samples from the previous run.\\\n",
    "**Please note:** from now on, the mixture we just inferred is stored in `rec`. Calling any of the following methods on the now empty mixture `mix` will result in an exception being raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1349431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.initialise()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4555f9",
   "metadata": {},
   "source": [
    "Let's have a look at this reconstruction. `dist` contains the realisation we just drew, with some useful methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a48d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[method_name for method_name in dir(rec)\n",
    "                  if callable(getattr(rec, method_name)) and not method_name.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3948db",
   "metadata": {},
   "source": [
    "`pdf` and `logpdf` take a 1D or 2D array and return, respectively, the probability and the log_probability of the inferred distribution, while `rvs` takes the number of desiderd samples and returns an array of draws. `cdf` and `logcdf` are the cumulative distribution function and its logarithm. These are, however, defined only for 1D distributions.\n",
    "\n",
    "We now want to evaluate the probability density over the interval $[x_{min},x_{max}]$.   \n",
    "**WARNING: FIGARO uses a coordinate change that is singular at boundaries. Be careful not to evaluate the mixture on or outside the boundaries. This will result in infs or NaNs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41c44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(x_min, x_max, 1002)[1:-1]\n",
    "p = rec.pdf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66135ae",
   "metadata": {},
   "source": [
    "Let's compare the reconstruction with the samples and with the true distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, b, t = plt.hist(samples, bins = int(np.sqrt(len(samples))), histtype = 'step', density = True, label = 'Samples')\n",
    "plt.plot(x, dist.pdf(x), color = 'red', lw = 0.7, label = 'Simulated')\n",
    "plt.plot(x, p, color = 'forestgreen', label = 'DPGMM')\n",
    "plt.legend(loc = 0, frameon = False)\n",
    "plt.grid(alpha = 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ea4d8",
   "metadata": {},
   "source": [
    "This is a *single* realisation from the Dirichlet Process. In order to properly explore the distribution space, we need a set of draws: therefore we need to repeat the exercise of training the DPGMM for every new sample we want. \n",
    "\n",
    "The DPGMM class contains a method that is a wrapper for the `for` loop we wrote before, `DPGMM.density_from_samples()`, which returns a realisation from the DP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee305ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_draws = 100\n",
    "draws = np.array([mix.density_from_samples(samples) for _ in tqdm(range(n_draws))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938571fa",
   "metadata": {},
   "source": [
    "Each call to `density_from_samples` reshuffles the samples and automatically initialise the mixture at the end.\n",
    "\n",
    "With the set of draws we have, we can compute median and credible regions for the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83decf63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probs = np.array([d.pdf(x) for d in draws])\n",
    "\n",
    "percentiles = [50, 5, 16, 84, 95]\n",
    "p = {}\n",
    "for perc in percentiles:\n",
    "    p[perc] = np.percentile(probs, perc, axis = 0)\n",
    "N = p[50].sum()*(x[1]-x[0])\n",
    "for perc in percentiles:\n",
    "    p[perc] = p[perc]/N\n",
    "\n",
    "n, b, t = plt.hist(samples, bins = int(np.sqrt(len(samples))), histtype = 'step', density = True, label = 'Samples')\n",
    "plt.fill_between(x, p[95], p[5], color = 'mediumturquoise', alpha = 0.5)\n",
    "plt.fill_between(x, p[84], p[16], color = 'darkturquoise', alpha = 0.5)\n",
    "plt.plot(x, dist.pdf(x), color = 'red', lw = 0.7, label = 'Simulated')\n",
    "plt.plot(x, p[50], color = 'steelblue', label = 'DPGMM')\n",
    "plt.legend(loc = 0, frameon = False)\n",
    "plt.grid(alpha = 0.6)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5633e9",
   "metadata": {},
   "source": [
    "The same plot can be obtained with the dedicated method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from figaro.utils import plot_median_cr\n",
    "plot_median_cr(draws,\n",
    "               injected = dist.pdf,\n",
    "               samples  = samples,\n",
    "               save     = False,\n",
    "               show     = True,\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f666b9",
   "metadata": {},
   "source": [
    "The draws are uncorrelated from each other. The autocorrelation function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7584dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from figaro.diagnostic import autocorrelation\n",
    "acf = autocorrelation(draws, bounds = [20, 40], save = False, show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b50ea7",
   "metadata": {},
   "source": [
    "Let's look at the entropy to assess the convergence of the recovered distribution to the injected one.\\\n",
    "In order to do so, we need to draw a single realisation, saving it every time we add a new sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb66aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.initialise()\n",
    "updated_mixture = []\n",
    "\n",
    "for s in tqdm(samples):\n",
    "    mix.add_new_point(s)\n",
    "    updated_mixture.append(mix.build_mixture())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1756f0c8",
   "metadata": {},
   "source": [
    "Once we have all the history of how the single distribution has been generated, the FIGARO package comes with a method that produces entropy plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73508bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from figaro.diagnostic import entropy\n",
    "\n",
    "S = entropy(updated_mixture, show = True, save = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb8115",
   "metadata": {},
   "source": [
    "It is also possible to compute an approximant of the entropy derivative to assess whether the distribution converged or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c181f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from figaro.diagnostic import plot_angular_coefficient\n",
    "\n",
    "ac = plot_angular_coefficient(S, show = True, save = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54a639f",
   "metadata": {},
   "source": [
    "When the number of accumulated samples is large enough to provide a good representation of the underlying distribution. the entropy reaches a plateau, and its derivative fluctuates around zero.\\\n",
    "Let's repeat the exercise with a larger number of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32696d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samps = 5000\n",
    "samples = dist.rvs(n_samps)\n",
    "\n",
    "mix.initialise()\n",
    "updated_mixture = []\n",
    "\n",
    "for s in tqdm(samples):\n",
    "    mix.add_new_point(s)\n",
    "    updated_mixture.append(mix.build_mixture())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3468dae3",
   "metadata": {},
   "source": [
    "Let's look at the recovered distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3821a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_median_cr([updated_mixture[-1]],\n",
    "               injected = dist.pdf,\n",
    "               samples  = samples,\n",
    "               save     = False,\n",
    "               show     = True\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be8482c",
   "metadata": {},
   "source": [
    "Entropy and angular coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd1412",
   "metadata": {},
   "outputs": [],
   "source": [
    "S  = entropy(updated_mixture, show = True, save = False)\n",
    "ac = plot_angular_coefficient(S, show = True, save = False, ac_expected = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0150154",
   "metadata": {},
   "source": [
    "With this number of samples, the angular coefficient starts fluctuating around 0 after ~3000 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f100193c",
   "metadata": {},
   "source": [
    "## Setting prior parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b93e8a",
   "metadata": {},
   "source": [
    "The prior distribution for means and covariances is the Normal-Inverse-Wishart distribution, which requires 4 parameters:\n",
    "* $\\nu$ is the number of degrees of freedom for the Inverse Wishart distribution,. It must be greater than $D+1$, where $D$ is the dimensionality of the distribution;\n",
    "* $k$ is the scale parameter for the multivariate Normal distribution;\n",
    "* $\\mu$ is the mean of the multivariate Normal distribution;\n",
    "* $\\Lambda$ is the expected value for the Inverse Wishart distribution, a covariance matrix.\n",
    "\n",
    "Setting these priors is nontrivial, given the fact that FIGARO operates, in a completely user-transparent way, a coordinate change and these distributions are defined in the transformed space.\n",
    "We suggest using the `figaro.utils.get_priors` method, which provides the user with an easy way to get the right parameters for instancing `figaro.mixture.DPGMM/HDPGMM` given their desired values in the natural space.\n",
    "\n",
    "The following list describes the arguments that can be passed to `get_priors` and their effect on the parameters:\n",
    "\n",
    "* `bounds` specifies the boundaries of the interval our reconstructed density will be defined, as in instancing the `DPGMM` class. It is the only mandatory argument;\n",
    "* `samples` contains the samples that will be used to reconstruct the probability density. They can be used to compute $\\mu$ and $\\Lambda$ if specific keyword arguments are not provided;\n",
    "* `mean` is the expected value for $\\mu$ in natural space, must be a $(D,)$-shaped array. If provided, it overrides `samples`;\n",
    "* `std` is the expected standard deviation for each dimension. It can be passed as a 1D array with shape ($D$,) or `double` (if `double`, it assumes that the same std has to be used for all dimensions). If provided, overrides `samples` in computing $\\Lambda$;\n",
    "* `cov` is the expected covariance matrix. It must be passed as 2D array with shape ($D$,$D$). If provided, overrides both `std` and `samples` in computing $\\Lambda$;\n",
    "* `df` corresponds to $\\nu$ and must be an integer value. It must be greater than $D+1$, otherwise the default value will be used;\n",
    "* `k` is the Normal scale parameter $k$ and it must be a positive `float`.\n",
    "\n",
    "With the exception of `bounds`, all the arguments are optional. Moreover, the user may decide to call `get_priors` with only some of them: the method will return default values for the others.\n",
    "\n",
    "The following list contains the default values for the prior paramters along with some hints on how to set them for a sensible run, still keeping in mind that, being the NIW a prior distribution, most of the information comes from the data themselves:\n",
    "\n",
    "* $\\mu$, by default, is set to the center of the ND interval, and for most cases it is ok to leave this unchanged. We suggest setting it to the samples mean (by passing the available samples via the `samples` keyword argument) while reconstructing the single-event posterior distributions of a hierarchical inference: this because the interval over which the hierarchical distribution is defined can be wider than the support of the single-event posterior distribution and the samples might be located away from the interval center;\n",
    "* $\\Lambda$ is by default set to be a diagonal matrix. In general, the default value is a good choice for most cases (samples that spans widely over the ND interval when the target distribution is expected to have only blunt features). If, on the other hand, the distribution is expected to show sharp features, like a relatively narrow Gaussian peak, we suggest using the `std` argument and setting it to something below the expected width of the feature. Finally, as above, for single-event posteriors in a hierarchical inference we suggest using the `samples` keyword argument and get this parameter from them;\n",
    "* $k$ controls the width of the Normal distribution. The default value is $10^{-2}$ and it is a good choice for most cases. In general, we suggest $ 10^{-4} \\lesssim k \\lesssim 10^{-1}$;\n",
    "* $\\nu$ controls the width of the Inverse Wishart distribution. It must be a positive integer and at least $D+2$, which is the default value. It can be interpreted as the *strenght* of the prior on the covariance matrix: greater values of $\\nu$ corresponds to giving more importance to the prior with respect to the likelihood (the samples). For most applications the default value is a good choice. We suggest to increase its value in those situations in which the samples are not available a priori (e.g. online reconstruction of probability densities) and the target distribution is expected to have a support much smaller than the whole ND interval, like for skymaps. We found that a good choice for this situation could be $2(D+2)$.\n",
    "\n",
    "`get_priors` returns a tuple which can be directly used to instance `DPGMM/HDPGMM`.\\\n",
    "We strongly recommend to use this method to convert the prior parameters. For the brave user that is still willing to pass its own tuple, the order in which the parameters must be passed to FIGARO is $(k,\\Lambda,\\nu,\\mu)$.\\\n",
    "Keep in mind that $\\mu$ and $\\Lambda$ must be in probit space: FIGARO is not able to distinguish between parameters in natural space or probit space, therefore no exceptions can be raised for this error.\\\n",
    "The behaviour of the code, in the case in which the user passes its own tuple and forgets to convert the parameters first, might be unpredictable.\n",
    "\n",
    "*Note:* A small fluctuation in $\\Lambda$ for subsequent calls with same argument is expected and it due to the fact that transforming a covariance matrix in probit space is nontrivial. In order to simplify the process, we decided to sample $10^4$ points from a multivariate Gaussian centered in $\\mu$ with the given covariance or std (still in natural space), transform the samples in probit space and use the covariance of the transformed samples as $\\Lambda$: from this, the fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f47791",
   "metadata": {},
   "source": [
    "The default values for $(k, \\Lambda,\\nu,\\mu)$ are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0918ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from figaro.utils import get_priors\n",
    "\n",
    "bounds = [[-5,5]]\n",
    "\n",
    "get_priors(bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c6475",
   "metadata": {},
   "source": [
    "One can directly call this method while instancing the `DPGMM` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3357da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = DPGMM(bounds, prior_pars = get_priors(bounds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df5b34",
   "metadata": {},
   "source": [
    "Priors from samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = norm(loc = -3, scale = 0.1).rvs(1000)\n",
    "\n",
    "get_priors(bounds, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eba2be",
   "metadata": {},
   "source": [
    "User-defined parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcdd5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_priors(bounds, mean = 1, std = 0.5, df = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e4bf0",
   "metadata": {},
   "source": [
    "FIGARO works also with multidimensional probability densities, as you will see in the following section. This method as well automatically adjust the default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1484c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-dimensional distribution\n",
    "bounds = [[0,1] for _ in range(4)]\n",
    "\n",
    "get_priors(bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04284a85",
   "metadata": {},
   "source": [
    "## Multidimensional probability density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579fd16e",
   "metadata": {},
   "source": [
    "Multidimensional probability densities can be inferred using the same functions.\n",
    "\n",
    "Let's generate some data from a bivariate Gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d04114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as mn\n",
    "from corner import corner\n",
    "\n",
    "n_samps = 1000\n",
    "samples = mn(np.zeros(2), np.identity(2)).rvs(n_samps)\n",
    "\n",
    "c = corner(samples, color = 'coral', labels = ['$x$','$y$'], hist_kwargs={'density':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ea3e7",
   "metadata": {},
   "source": [
    "The only difference with the previous case is that the mixture needs to be instantiated specifying the bounds for both dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ce7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = -5\n",
    "x_max = 5\n",
    "y_min = -5\n",
    "y_max = 5\n",
    "\n",
    "mix_2d = DPGMM([[x_min, x_max],[y_min, y_max]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d4b75",
   "metadata": {},
   "source": [
    "The inference runs exactly as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2982a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in tqdm(samples):\n",
    "    mix_2d.add_new_point(s)\n",
    "rec = mix_2d.build_mixture()\n",
    "mix_2d.initialise()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d749c",
   "metadata": {},
   "source": [
    "Let's compare the initial samples with a set of samples drawn from the recovered distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02166b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_samples = rec.rvs(n_samps)\n",
    "\n",
    "\n",
    "c = corner(samples, color = 'coral', labels = ['$x$','$y$'], hist_kwargs={'density':True, 'label':'$\\mathrm{Samples}$'})\n",
    "c = corner(mix_samples, fig = c, color = 'dodgerblue', labels = ['$x$','$y$'], hist_kwargs={'density':True, 'label':'$\\mathrm{DPGMM}$'})\n",
    "l = plt.legend(loc = 0,frameon = False,fontsize = 15, bbox_to_anchor = (1-0.05, 1.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c60f55",
   "metadata": {},
   "source": [
    "Multiple draws:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4318e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_draws = 100\n",
    "draws = []\n",
    "\n",
    "for _ in tqdm(range(n_draws)):\n",
    "    draws.append(mix_2d.density_from_samples(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c2168",
   "metadata": {},
   "source": [
    "## Hierarchical inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0000e0",
   "metadata": {},
   "source": [
    "Let's assume to have a set of samples $\\{x_1,\\ldots,x_k\\}$ from some probability density $F(x)$. Around each $x_i$, another process generated a set of samples $\\mathbf{y}_i = \\{y_1^i,\\ldots,y_n^i\\}$ according to some distribution $f_i(y|x_i)$.   \n",
    "To give a bit of context, $\\{x_1,\\ldots,x_k\\}$ could be the true masses of the black holes observed by LIGO and Virgo drawn from the mass function $F(x)$ and each $\\mathbf{y}_i$ could be the set of single-event primary mass posterior samples drawn from the posterior samples $f_i(y|x_i)$.\n",
    "\n",
    "In this section we'll see how to use FIGARO to infer $F(x)$ using $\\{\\mathbf{y}_1,\\ldots,\\mathbf{y}_k\\}$.\n",
    "In the following example, both $F(x)$ and $f_i(y|x_i)$ are Gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 30\n",
    "sigma = 5\n",
    "n_evs = 1000\n",
    "n_post_samps = 100\n",
    "\n",
    "mass_function = norm(mu, sigma)\n",
    "true_masses = mass_function.rvs(n_evs)\n",
    "\n",
    "single_event_posteriors = [norm(norm(M, s).rvs(), s).rvs(n_post_samps) for M, s in zip(true_masses, np.random.uniform(1,3, size = len(true_masses)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a3b62",
   "metadata": {},
   "source": [
    "First of all, we need to reconstruct the $k$ probability densities $f_i$. For each $y_i$, we can use the DPGMM class.\n",
    "A proper analysis would require to draw multiple realisations for each posterior distribution. In this example, for the sake of time, we will draw only a handful of realisations for each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d87c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_draws = 10\n",
    "x_min = 1\n",
    "x_max = 70\n",
    "mix = DPGMM([[x_min, x_max]])\n",
    "\n",
    "posteriors = []\n",
    "for event in tqdm(single_event_posteriors, desc = 'Events'):\n",
    "    draws = []\n",
    "    for _ in range(n_draws):\n",
    "        draws.append(mix.density_from_samples(event))\n",
    "    posteriors.append(draws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04617802",
   "metadata": {},
   "source": [
    "Once we have the single-event posterior reconstructions, we need the HDPGMM class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from figaro.mixture import HDPGMM\n",
    "hier_mix = HDPGMM([[x_min, x_max]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0686eff",
   "metadata": {},
   "source": [
    "The methods for this new class are the same we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0fb737",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_draws_hier = 100\n",
    "hier_draws = []\n",
    "\n",
    "for _ in tqdm(range(n_draws_hier)):\n",
    "    hier_draws.append(hier_mix.density_from_samples(posteriors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57358804",
   "metadata": {},
   "source": [
    "In the same fashion, we can plot the recovered distribution using the dedicated method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b6a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_median_cr(hier_draws,\n",
    "               samples  = true_masses,\n",
    "               injected = mass_function.pdf,\n",
    "               show     = True,\n",
    "               hierarchical = True\n",
    "               )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
