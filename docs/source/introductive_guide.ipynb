{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIGARO: an introductive guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to use FIGARO, *Fast Inference for GW Astronomy, Research & Observations*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D probability density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start from a simple problem: inferring a 1D probability density given a set of samples drawn from it.\n",
    "Let's draw some samples from a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, uniform\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from figaro import plot_settings\n",
    "\n",
    "mu = 30\n",
    "sigma = 3\n",
    "n_samps = 1000\n",
    "dist = norm(mu, sigma) \n",
    "\n",
    "samples = dist.rvs(n_samps)\n",
    "\n",
    "n, b, p = plt.hist(samples, histtype = 'step', density = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIGARO contains a class designed to infer probability densities given a set of samples.\n",
    "\n",
    "In order to instantiate the class, we need to specify the boundaries of the distribution.\n",
    "We will assume that our probability density is bounded between 10 and 50. Moreover, we will make use of the available samples to set some reasonable priors for the mean and variance of each Gaussian component fo the mixture making use of the `get_priors` method. More details on this function are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figaro.mixture import DPGMM\n",
    "from figaro.utils import get_priors\n",
    "\n",
    "x_min = 10\n",
    "x_max = 50\n",
    "bounds = [x_min, x_max]\n",
    "prior_pars = get_priors(bounds = bounds, samples = samples)\n",
    "\n",
    "mix = DPGMM(bounds, prior_pars = prior_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that the algorithm *learns* the shape of the probability density from the available samples, one at a time: every new sample adds a piece of information to the inference. Therefore, we need to pass the samples to our mixture one at a time in order to draw a single realisation of the Dirichlet Process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in tqdm(samples):\n",
    "    mix.add_new_point(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our mixture knows the shape of the distribution, we can build the probability density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = mix.build_mixture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting again with a new inference the mixture must be initialised, otherwise it will remember the samples from the previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.initialise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note:** the mixture we just inferred is stored in `rec`. Calling any of the following methods on the now empty mixture `mix` will raise an exception.\\\n",
    "Let's have a look at this reconstruction: `rec` contains the realisation we just drew, with some useful methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[method_name for method_name in dir(rec)\n",
    "                  if callable(getattr(rec, method_name)) and not method_name.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pdf` and `logpdf` take a 1D or 2D array and return, respectively, the probability and the log_probability of the inferred distribution, while `rvs` takes the number of desiderd samples and returns an array of draws. `cdf` and `logcdf` are the cumulative distribution function and its logarithm. These are, however, defined only for 1D distributions. `fast_pdf` and `fast_logpdf` take a single point (1D array) and return the pdf/logpdf for that specific point without relying on NumPy. These functions are thought for MCMCs and nested samplings. `marginalise` and `condition` makes use of the properties of the multivariate Gaussian distribution to analytically marginalise or condition the inferred distribution on some of its variables. \n",
    "\n",
    "An instance of the `figaro.mixture.mixture` class behave mostly like a `scipy.stats` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = 30\n",
    "print(rec.pdf(pt), rec.logpdf(pt), rec.cdf(pt), rec.rvs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to evaluate the probability density over the interval $[x_{min},x_{max}]$.   \n",
    "**WARNING: FIGARO uses a coordinate change that is singular at boundaries. The distribution is defined to be zero outside bounds.** This transformation is used to ensure that the conditions for the GMM to be a good approximant for arbitrary probability densities ([Nguyen et al 2012](https://www.tandfonline.com/doi/full/10.1080/25742558.2020.1750861)) are met. If you don't want to make use of this transformation, set `probit = False` while instantiating the DPGMM class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(x_min, x_max, 1000)\n",
    "p = rec.pdf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the reconstruction with the samples and with the true distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, b, t = plt.hist(samples, bins = int(np.sqrt(len(samples))), histtype = 'step', density = True, label = 'Samples')\n",
    "plt.plot(x, dist.pdf(x), color = 'red', lw = 0.7, label = 'Simulated')\n",
    "plt.plot(x, p, color = 'forestgreen', label = 'DPGMM')\n",
    "plt.legend(loc = 0, frameon = False)\n",
    "plt.grid(alpha = 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a *single* realisation from the Dirichlet Process. In order to properly explore the distribution space, we need a set of draws: therefore we need to repeat the exercise of training the DPGMM for every new sample we want. \n",
    "\n",
    "The DPGMM class contains a method that is a wrapper for the `for` loop we wrote before, `DPGMM.density_from_samples()`, which returns a realisation from the DP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_draws = 100\n",
    "draws = np.array([mix.density_from_samples(samples) for _ in tqdm(range(n_draws))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each call to `density_from_samples` reshuffles the samples and automatically initialise the mixture at the end.\n",
    "\n",
    "With the set of draws we have, we can compute median and credible regions for the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probs = np.array([d.pdf(x) for d in draws])\n",
    "\n",
    "percentiles = [50, 5, 16, 84, 95]\n",
    "p = {}\n",
    "for perc in percentiles:\n",
    "    p[perc] = np.percentile(probs, perc, axis = 0)\n",
    "N = p[50].sum()*(x[1]-x[0])\n",
    "for perc in percentiles:\n",
    "    p[perc] = p[perc]/N\n",
    "\n",
    "n, b, t = plt.hist(samples, bins = int(np.sqrt(len(samples))), histtype = 'step', density = True, label = 'Samples')\n",
    "plt.fill_between(x, p[95], p[5], color = 'mediumturquoise', alpha = 0.5)\n",
    "plt.fill_between(x, p[84], p[16], color = 'darkturquoise', alpha = 0.5)\n",
    "plt.plot(x, dist.pdf(x), color = 'red', lw = 0.7, label = 'Simulated')\n",
    "plt.plot(x, p[50], color = 'steelblue', label = 'DPGMM')\n",
    "plt.legend(loc = 0, frameon = False)\n",
    "plt.grid(alpha = 0.6)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same plot can be obtained with the dedicated method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figaro.plot import plot_median_cr\n",
    "plot_median_cr(draws,\n",
    "               injected = dist.pdf,\n",
    "               samples  = samples,\n",
    "               save     = False,\n",
    "               show     = True,\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The draws are uncorrelated from each other. The autocorrelation function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figaro.diagnostic import autocorrelation\n",
    "acf = autocorrelation(draws, bounds = [20, 40], save = False, show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the entropy to assess the convergence of the recovered distribution to the injected one.\\\n",
    "In order to do so, we need to draw a single realisation, saving it every time we add a new sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.initialise()\n",
    "updated_mixture = []\n",
    "\n",
    "for s in tqdm(samples):\n",
    "    mix.add_new_point(s)\n",
    "    updated_mixture.append(mix.build_mixture())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have all the history of how the single distribution has been generated, the FIGARO package comes with a method that produces entropy plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figaro.diagnostic import entropy\n",
    "\n",
    "S = entropy(updated_mixture, show = True, save = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to compute an approximant of the entropy derivative to assess whether the distribution converged or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figaro.diagnostic import plot_angular_coefficient\n",
    "\n",
    "ac = plot_angular_coefficient(S, show = True, save = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of accumulated samples is large enough to provide a good representation of the underlying distribution. the entropy reaches a plateau, and its derivative fluctuates around zero.\\\n",
    "Let's repeat the exercise with a larger number of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samps = 5000\n",
    "samples = dist.rvs(n_samps)\n",
    "\n",
    "mix.initialise()\n",
    "updated_mixture = []\n",
    "\n",
    "for s in tqdm(samples):\n",
    "    mix.add_new_point(s)\n",
    "    updated_mixture.append(mix.build_mixture())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the recovered distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_median_cr([updated_mixture[-1]],\n",
    "               injected = dist.pdf,\n",
    "               samples  = samples,\n",
    "               save     = False,\n",
    "               show     = True,\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy and angular coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S  = entropy(updated_mixture, show = True, save = False)\n",
    "ac = plot_angular_coefficient(S, show = True, save = False, ac_expected = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this number of samples, the angular coefficient starts fluctuating around 0 after ~3000 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting prior parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior distribution for means and covariances is the Normal-Inverse-Wishart distribution, which requires 4 parameters:\n",
    "* $\\nu$ is the number of degrees of freedom for the Inverse-Wishart distribution,. It must be greater than $D+1$, where $D$ is the dimensionality of the distribution;\n",
    "* $k$ is the scale parameter for the multivariate Normal distribution;\n",
    "* $\\mu$ is the mean of the multivariate Normal distribution;\n",
    "* $\\Lambda$ is the expected value for the Inverse-Wishart distribution, a covariance matrix.\n",
    "\n",
    "The `figaro.utils.get_priors` method provides the user with an easy way to get the right parameters for instancing `figaro.mixture.DPGMM/HDPGMM` given their desired values in the natural space. The returned tuple can be directly used to instance `DPGMM/HDPGMM`.\n",
    "\n",
    "The following list describes the arguments that can be passed to `get_priors` and their effect on the parameters:\n",
    "\n",
    "* `bounds` specifies the boundaries of the interval our reconstructed density will be defined, as in instancing the `DPGMM` class. It is the only mandatory argument;\n",
    "* `samples` contains the samples that will be used to reconstruct the probability density. They can be used to compute $\\mu$ and $\\Lambda$ if specific keyword arguments are not provided;\n",
    "* `mean` is the expected value for $\\mu$ in natural space, must be a $(D,)$-shaped array. If provided, it overrides `samples`. The default value is the center of the `bounds` box;\n",
    "* `std` is the expected standard deviation for each dimension. It can be passed as a 1D array with shape ($D$,) or `double` (if `double`, it assumes that the same std has to be used for all dimensions). If provided, overrides `samples` in computing $\\Lambda$;\n",
    "* `df` corresponds to $\\nu$ and must be an integer value. It must be greater than $D+1$, otherwise the default value will be used (we recommend not to change this parameter);\n",
    "* `k` is the Normal scale parameter $k$ and it must be a positive `float` (we recommend not to change this parameter);\n",
    "* `a` is the shape parameter for the Inverse Gamma distribution used as prior for the (H)DPGMM reconstruction;\n",
    "* `scale` is a parameter used to determine the expected standard deviation in absence of the `std` parameter. If `samples` are provided, the expected std is set to be `std(samples)/scale`. Otherwise, if no parameters are set, the expected std is the box width divided by `scale`;\n",
    "* `probit` should be set to `False` if the probit transformation is not used, to ensure consistency (default is `True`);\n",
    "* `hierarchical` must be set to `True` while getting the prior parameters for the HDPGMM class.\n",
    "\n",
    "With the exception of `bounds`, all the arguments are optional. Moreover, the user may decide to call `get_priors` with only some of them: the method will return default values for the others. As a rule of thumb, the best practice is to simply call `get_priors(bounds = bounds, samples = samples)`, adding the specifications for the hierarchical inference or to disable the probit transformation, if needed, and play with either the `scale` or `std` parameter only. \n",
    "\n",
    "*Note:* A small fluctuation in $\\Lambda$ for subsequent calls with same argument is expected and it due to the fact that transforming a covariance matrix in probit space is nontrivial. In order to simplify the process, we decided to sample $10^4$ points from a multivariate Gaussian centered in $\\mu$ with the given covariance or std (still in natural space), transform the samples in probit space and use the covariance of the transformed samples as $\\Lambda$: from this, the fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can directly call this method while instancing the `DPGMM` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = DPGMM(bounds, prior_pars = get_priors(bounds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Priors from samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = norm(loc = -3, scale = 0.1).rvs(1000)\n",
    "\n",
    "get_priors(bounds, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-defined parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_priors(bounds, mean = 1, std = 0.5, df = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that $\\mu$ and $\\Lambda$ are different from the proposed values due to the coordinate change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIGARO works also with multidimensional probability densities, as you will see in the following section. This method as well automatically adjust the default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-dimensional distribution\n",
    "bounds = [[0,1] for _ in range(4)]\n",
    "\n",
    "get_priors(bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multidimensional probability density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multidimensional probability densities can be inferred using the same functions.\n",
    "\n",
    "Let's generate some data from a bivariate Gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as mn\n",
    "from corner import corner\n",
    "\n",
    "n_samps = 1000\n",
    "samples = mn(np.zeros(2), np.identity(2)).rvs(n_samps)\n",
    "\n",
    "c = corner(samples, color = 'coral', labels = ['$x$','$y$'], hist_kwargs={'density':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference with the previous case is that the mixture needs to be instantiated specifying the bounds for both dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = -5\n",
    "x_max = 5\n",
    "y_min = -5\n",
    "y_max = 5\n",
    "\n",
    "bounds = [[x_min, x_max],[y_min, y_max]]\n",
    "\n",
    "mix_2d = DPGMM(bounds, prior_pars = get_priors(bounds, samples, scale = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference runs exactly as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in tqdm(samples):\n",
    "    mix_2d.add_new_point(s)\n",
    "rec = mix_2d.build_mixture()\n",
    "mix_2d.initialise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the initial samples with a set of samples drawn from the recovered distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_samples = rec.rvs(n_samps)\n",
    "\n",
    "\n",
    "c = corner(samples, color = 'coral', labels = ['$x$','$y$'], hist_kwargs={'density':True, 'label':'$\\mathrm{Samples}$'})\n",
    "c = corner(mix_samples, fig = c, color = 'dodgerblue', labels = ['$x$','$y$'], hist_kwargs={'density':True, 'label':'$\\mathrm{DPGMM}$'})\n",
    "l = plt.legend(loc = 0,frameon = False,fontsize = 15, bbox_to_anchor = (1-0.05, 1.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple draws:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_draws = 100\n",
    "draws = []\n",
    "\n",
    "for _ in tqdm(range(n_draws)):\n",
    "    draws.append(mix_2d.density_from_samples(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the inferred distribution using the `figaro.plot.plot_multidim` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figaro.plot import plot_multidim\n",
    "\n",
    "plot_multidim(draws,\n",
    "              samples = samples,\n",
    "              save = False,\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume to have a set of samples $\\{x_1,\\ldots,x_k\\}$ from some probability density $F(x)$. Around each $x_i$, another process generated a set of samples $\\mathbf{y}_i = \\{y_1^i,\\ldots,y_n^i\\}$ according to some distribution $f_i(y|x_i)$.   \n",
    "To give a bit of context, $\\{x_1,\\ldots,x_k\\}$ could be the true masses of the black holes observed by LIGO and Virgo drawn from the mass function $F(x)$ and each $\\mathbf{y}_i$ could be the set of single-event primary mass posterior samples drawn from the posterior samples $f_i(y|x_i)$.\n",
    "\n",
    "In this section we'll see how to use FIGARO to infer $F(x)$ using $\\{\\mathbf{y}_1,\\ldots,\\mathbf{y}_k\\}$.\n",
    "In the following example, both $F(x)$ and $f_i(y|x_i)$ are Gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 30\n",
    "sigma = 5\n",
    "n_evs = 100\n",
    "n_post_samps = 100\n",
    "\n",
    "mass_function = norm(mu, sigma)\n",
    "true_masses = mass_function.rvs(n_evs)\n",
    "\n",
    "single_event_posteriors = [norm(norm(M, s).rvs(), s).rvs(n_post_samps) for M, s in zip(true_masses, np.random.uniform(1,3, size = len(true_masses)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to reconstruct the $k$ probability densities $f_i$. For each $y_i$, we can use the DPGMM class.\n",
    "A proper analysis would require to draw multiple realisations for each posterior distribution. In this example, for the sake of time, we will draw only a handful of realisations for each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_draws = 10\n",
    "x_min = 1\n",
    "x_max = 70\n",
    "bounds = [[x_min, x_max]]\n",
    "\n",
    "posteriors = []\n",
    "for event in tqdm(single_event_posteriors, desc = 'Events'):\n",
    "    draws = []\n",
    "    mix = DPGMM(bounds, prior_pars = get_priors(bounds, samples = event))\n",
    "    for _ in range(n_draws):\n",
    "        draws.append(mix.density_from_samples(event))\n",
    "    posteriors.append(draws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the single-event posterior reconstructions, we need the HDPGMM class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figaro.mixture import HDPGMM\n",
    "bounds = [[x_min, x_max]]\n",
    "hier_mix = HDPGMM(bounds, prior_pars = get_priors(bounds, samples = single_event_posteriors, hierarchical = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods for this new class are the same we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_draws_hier = 100\n",
    "hier_draws = []\n",
    "\n",
    "for _ in tqdm(range(n_draws_hier)):\n",
    "    hier_draws.append(hier_mix.density_from_samples(posteriors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same fashion, we can plot the recovered distribution using the dedicated method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_median_cr(hier_draws,\n",
    "               samples  = true_masses,\n",
    "               injected = mass_function.pdf,\n",
    "               show     = True,\n",
    "               hierarchical = True\n",
    "               )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
